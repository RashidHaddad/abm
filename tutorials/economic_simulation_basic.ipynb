{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODmQYtRCgWcI"
      },
      "source": [
        "Copyright (c) 2020, salesforce.com, inc.  \n",
        "All rights reserved.  \n",
        "SPDX-License-Identifier: BSD-3-Clause  \n",
        "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYIoK-Z0gWcK"
      },
      "source": [
        "### Colab\n",
        "\n",
        "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvX-n1togWcK"
      },
      "source": [
        "# Welcome to Foundation!\n",
        "\n",
        "Foundation is the name of the economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). This is the first of several tutorials designed to explain how Foundation works and how it can be used to build simulation environments for studying economic problems.\n",
        "\n",
        "Just to orient you a bit, Foundation is specially designed for modeling economies in spatial, 2D grid worlds. The AI Economist paper uses a scenario with 4 agents in a world with *Stone* and *Wood*, which can be *collected*, *traded*, and used to build *Houses*. Here's a (nicely rendered) example of what such an environment looks like:\n",
        "\n",
        "![Foundation snapshot](https://github.com/salesforce/ai-economist/blob/master/tutorials/assets/foundation_snapshot_rendered.jpg?raw=1)\n",
        "\n",
        "This image just shows what you might see spatially. Behind the scenes, agents have inventories of Stone, Wood, and *Coin*, which they can exchange through a commodities marketplace. In addition, they periodically pay taxes on income earned through trading and building.\n",
        "\n",
        "**We've open-sourced Foundation to foster transparency and to enable others to build on it!** With that goal in mind, this first tutorial should give you enough to see how to create the type of simulation environment described above and how to interact with it. If you're interested to learn how it all works and how to build on it, make sure to check out the advanced tutorial as well! If, after that, you want to understand more about designing the simulation around economic problems, check out our tutorial explaining how the AI Economist uses Foundation to study the optimal taxation problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTldNauogWcK"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this **basic** tutorial, we will demonstrate the basics of how to create an instance of a simulation environment and how to interact with it.\n",
        "\n",
        "We will cover the following:\n",
        "1. Markov Decision Processes\n",
        "2. Creating a Simulation Environment (a Scenario Instance)\n",
        "3. Interacting with the Simulation\n",
        "4. Sampling and Visualizing an Episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhz4s69VgWcK"
      },
      "source": [
        "## Dependencies:\n",
        "You can install the ai-economist package using\n",
        "- the pip package manager OR\n",
        "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoMHd-JCgWcL",
        "outputId": "9c510a2b-d484-48b7-cb3b-1e1d23d91ebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-economist'...\n",
            "remote: Enumerating objects: 1003, done.\u001b[K\n",
            "remote: Counting objects: 100% (256/256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 1003 (delta 220), reused 211 (delta 211), pack-reused 747 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1003/1003), 2.09 MiB | 7.29 MiB/s, done.\n",
            "Resolving deltas: 100% (635/635), done.\n",
            "/content/ai-economist\n",
            "Obtaining file:///content/ai-economist\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting appdirs==1.4.4 (from ai-economist==1.7.1)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting appnope==0.1.2 (from ai-economist==1.7.1)\n",
            "  Downloading appnope-0.1.2-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting argon2-cffi==20.1.0 (from ai-economist==1.7.1)\n",
            "  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting astroid==2.5.6 (from ai-economist==1.7.1)\n",
            "  Downloading astroid-2.5.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting async-generator==1.10 (from ai-economist==1.7.1)\n",
            "  Downloading async_generator-1.10-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting attrs==21.2.0 (from ai-economist==1.7.1)\n",
            "  Downloading attrs-21.2.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (0.2.0)\n",
            "Collecting beautifulsoup4==4.9.3 (from ai-economist==1.7.1)\n",
            "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting black==21.5b1 (from ai-economist==1.7.1)\n",
            "  Downloading black-21.5b1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting bleach==3.3.0 (from ai-economist==1.7.1)\n",
            "  Downloading bleach-3.3.0-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Collecting bs4==0.0.1 (from ai-economist==1.7.1)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting certifi==2020.12.5 (from ai-economist==1.7.1)\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cffi==1.14.5 (from ai-economist==1.7.1)\n",
            "  Downloading cffi-1.14.5.tar.gz (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.1/475.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chardet==4.0.0 (from ai-economist==1.7.1)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting click==8.0.1 (from ai-economist==1.7.1)\n",
            "  Downloading click-8.0.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cycler==0.10.0 (from ai-economist==1.7.1)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==5.0.9 (from ai-economist==1.7.1)\n",
            "  Downloading decorator-5.0.9-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (0.7.1)\n",
            "Collecting entrypoints==0.3 (from ai-economist==1.7.1)\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting et-xmlfile==1.1.0 (from ai-economist==1.7.1)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting flake8==3.9.2 (from ai-economist==1.7.1)\n",
            "  Downloading flake8-3.9.2-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting GPUtil==1.4.0 (from ai-economist==1.7.1)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting idna==2.10 (from ai-economist==1.7.1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting iniconfig==1.1.1 (from ai-economist==1.7.1)\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting ipykernel==5.5.5 (from ai-economist==1.7.1)\n",
            "  Downloading ipykernel-5.5.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting ipython==7.31.1 (from ai-economist==1.7.1)\n",
            "  Downloading ipython-7.31.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (0.2.0)\n",
            "Collecting ipywidgets==7.6.3 (from ai-economist==1.7.1)\n",
            "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting isort==5.8.0 (from ai-economist==1.7.1)\n",
            "  Downloading isort-5.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jedi==0.18.0 (from ai-economist==1.7.1)\n",
            "  Downloading jedi-0.18.0-py2.py3-none-any.whl.metadata (20 kB)\n",
            "Collecting Jinja2==3.0.1 (from ai-economist==1.7.1)\n",
            "  Downloading Jinja2-3.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting jsonschema==3.2.0 (from ai-economist==1.7.1)\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting jupyter==1.0.0 (from ai-economist==1.7.1)\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Requirement already satisfied: jupyter-client==6.1.12 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (6.1.12)\n",
            "Collecting jupyter-console==6.4.0 (from ai-economist==1.7.1)\n",
            "  Downloading jupyter_console-6.4.0-py3-none-any.whl.metadata (951 bytes)\n",
            "Collecting jupyter-core==4.7.1 (from ai-economist==1.7.1)\n",
            "  Downloading jupyter_core-4.7.1-py3-none-any.whl.metadata (759 bytes)\n",
            "Collecting jupyterlab-pygments==0.1.2 (from ai-economist==1.7.1)\n",
            "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl.metadata (329 bytes)\n",
            "Collecting jupyterlab-widgets==1.0.0 (from ai-economist==1.7.1)\n",
            "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting kiwisolver==1.3.1 (from ai-economist==1.7.1)\n",
            "  Downloading kiwisolver-1.3.1.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lazy-object-proxy==1.6.0 (from ai-economist==1.7.1)\n",
            "  Downloading lazy-object-proxy-1.6.0.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lz4==3.1.3 (from ai-economist==1.7.1)\n",
            "  Downloading lz4-3.1.3.tar.gz (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.0/159.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting MarkupSafe==2.0.1 (from ai-economist==1.7.1)\n",
            "  Downloading MarkupSafe-2.0.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==3.2.1 (from ai-economist==1.7.1)\n",
            "  Downloading matplotlib-3.2.1.tar.gz (40.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib-inline==0.1.2 (from ai-economist==1.7.1)\n",
            "  Downloading matplotlib_inline-0.1.2-py3-none-any.whl.metadata (380 bytes)\n",
            "Collecting mccabe==0.6.1 (from ai-economist==1.7.1)\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mistune==0.8.4 (from ai-economist==1.7.1)\n",
            "  Downloading mistune-0.8.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting mypy-extensions==0.4.3 (from ai-economist==1.7.1)\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting nbclient==0.5.3 (from ai-economist==1.7.1)\n",
            "  Downloading nbclient-0.5.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting nbconvert==6.0.7 (from ai-economist==1.7.1)\n",
            "  Downloading nbconvert-6.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting nbformat==5.1.3 (from ai-economist==1.7.1)\n",
            "  Downloading nbformat-5.1.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting nest-asyncio==1.5.1 (from ai-economist==1.7.1)\n",
            "  Downloading nest_asyncio-1.5.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting notebook==6.4.1 (from ai-economist==1.7.1)\n",
            "  Downloading notebook-6.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting numpy==1.21.0 (from ai-economist==1.7.1)\n",
            "  Downloading numpy-1.21.0.zip (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openpyxl==3.0.7 (from ai-economist==1.7.1)\n",
            "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting packaging==20.9 (from ai-economist==1.7.1)\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pandas==1.2.4 (from ai-economist==1.7.1)\n",
            "  Downloading pandas-1.2.4.tar.gz (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandocfilters==1.4.3 (from ai-economist==1.7.1)\n",
            "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting parso==0.8.2 (from ai-economist==1.7.1)\n",
            "  Downloading parso-0.8.2-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pathspec==0.8.1 (from ai-economist==1.7.1)\n",
            "  Downloading pathspec-0.8.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pexpect==4.8.0 (from ai-economist==1.7.1)\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (0.7.5)\n",
            "Collecting Pillow==9.0.1 (from ai-economist==1.7.1)\n",
            "  Downloading Pillow-9.0.1.tar.gz (49.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pluggy==0.13.1 (from ai-economist==1.7.1)\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting prometheus-client==0.10.1 (from ai-economist==1.7.1)\n",
            "  Downloading prometheus_client-0.10.1-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting prompt-toolkit==3.0.18 (from ai-economist==1.7.1)\n",
            "  Downloading prompt_toolkit-3.0.18-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.11/dist-packages (from ai-economist==1.7.1) (0.7.0)\n",
            "Collecting py==1.10.0 (from ai-economist==1.7.1)\n",
            "  Downloading py-1.10.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pycodestyle==2.7.0 (from ai-economist==1.7.1)\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pycparser==2.20 (from ai-economist==1.7.1)\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting pycryptodome==3.10.1 (from ai-economist==1.7.1)\n",
            "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting pyflakes==2.3.1 (from ai-economist==1.7.1)\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting Pygments==2.9.0 (from ai-economist==1.7.1)\n",
            "  Downloading Pygments-2.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pylint==2.8.2 (from ai-economist==1.7.1)\n",
            "  Downloading pylint-2.8.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pyparsing==2.4.7 (from ai-economist==1.7.1)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pyrsistent==0.17.3 (from ai-economist==1.7.1)\n",
            "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest==6.2.4 (from ai-economist==1.7.1)\n",
            "  Downloading pytest-6.2.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting python-dateutil==2.8.1 (from ai-economist==1.7.1)\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pytz==2021.1 (from ai-economist==1.7.1)\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pyyaml==5.4.1 (from ai-economist==1.7.1)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "\n",
            "Restarting the Python runtime! Please (re-)run the cells below.\n"
          ]
        }
      ],
      "source": [
        "import os, signal, sys, time\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/salesforce/ai-economist.git\n",
        "\n",
        "    %cd ai-economist\n",
        "    !pip install -e .\n",
        "\n",
        "    # Restart the Python runtime to automatically use the installed packages\n",
        "    print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
        "    time.sleep(1)\n",
        "    os.kill(os.getpid(), signal.SIGKILL)\n",
        "else:\n",
        "    !pip install ai-economist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lUX5tUdgWcL"
      },
      "outputs": [],
      "source": [
        "# Import foundation\n",
        "from ai_economist import foundation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poX3kTYfgWcL"
      },
      "outputs": [],
      "source": [
        "# Change directory to the tutorials folder\n",
        "import os, sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    os.chdir(\"/content/ai-economist/tutorials\")\n",
        "else:\n",
        "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1tZl-KEgWcL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from utils import plotting  # plotting utilities for visualizing env. state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isqNTpF8gWcM"
      },
      "source": [
        "# 1. Markov Decision Process\n",
        "\n",
        "Formally, our economic simulation is a key part of a Markov Decision Process (MDP).\n",
        "\n",
        "MDPs describe episodes in which agents interact with a stateful environment in a continuous feedback loop. At each timestep, agents receive an observation and use a policy to choose actions. The environment then advances to a new state, using the old state and the chosen actions. The agents then receive new observations and rewards. This process repeats over $T$ timesteps (possibly infinite).\n",
        "\n",
        "The goal of each agent is to maximize its expected sum of future (discounted) rewards, by finding its optimal policy. Intuitively, this means that an agent needs to understand which (sequence of) actions lead to high rewards (in expectation).\n",
        "\n",
        "### References\n",
        "\n",
        "For more information on reinforcement learning and MDPs, check out:\n",
        "\n",
        "- Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction. [http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCAt-2lgWcM"
      },
      "source": [
        "# 2. Creating a Simulation Environment (a Scenario Instance)\n",
        "\n",
        "The Scenario class implements an economic simulation with multiple agents and (optionally) a social planner.\n",
        "\n",
        "Scenarios provide a high-level gym-style API that lets agents interact with it. The API supports multi-agent observations, actions, etc.\n",
        "\n",
        "Each Scenario is stateful and implements two main methods:\n",
        "\n",
        "- __step__, which advances the simulation to the next state, and\n",
        "- __reset__, which puts the simulation back in an initial state.\n",
        "\n",
        "Each Scenario is customizable: you can specify options in a dictionary. Here is an example for a scenario with 4 agents:\n",
        "\n",
        "**Note: This config dictionary will likely seem fairly incomprehensible at this point in the tutorials. Don't worry. The advanced tutorial offers much more context. This is just to get things started and to provide a reference for how to create a \"free market\" economy from the AI Economist.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOeiPyAcgWcM"
      },
      "outputs": [],
      "source": [
        "# Define the configuration of the environment that will be built\n",
        "\n",
        "env_config = {\n",
        "    # ===== SCENARIO CLASS =====\n",
        "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
        "    # The environment object will be an instance of the Scenario class.\n",
        "    'scenario_name': 'layout_from_file/simple_wood_and_stone',\n",
        "\n",
        "    # ===== COMPONENTS =====\n",
        "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
        "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
        "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
        "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
        "    'components': [\n",
        "        # (1) Building houses\n",
        "        ('Build', {'skill_dist': \"pareto\", 'payment_max_skill_multiplier': 3}),\n",
        "        # (2) Trading collectible resources\n",
        "        ('ContinuousDoubleAuction', {'max_num_orders': 5}),\n",
        "        # (3) Movement and resource collection\n",
        "        ('Gather', {}),\n",
        "    ],\n",
        "\n",
        "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
        "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
        "    'env_layout_file': 'quadrant_25x25_20each_30clump.txt',\n",
        "    'starting_agent_coin': 10,\n",
        "    'fixed_four_skill_and_loc': True,\n",
        "\n",
        "    # ===== STANDARD ARGUMENTS ======\n",
        "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
        "    'n_agents': 4,          # Number of non-planner agents (must be > 1)\n",
        "    'world_size': [25, 25], # [Height, Width] of the env world\n",
        "    'episode_length': 1000, # Number of timesteps per episode\n",
        "\n",
        "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
        "    # Otherwise, the policy selects only 1 action.\n",
        "    'multi_action_mode_agents': False,\n",
        "    'multi_action_mode_planner': True,\n",
        "\n",
        "    # When flattening observations, concatenate scalar & vector observations before output.\n",
        "    # Otherwise, return observations with minimal processing.\n",
        "    'flatten_observations': False,\n",
        "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
        "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
        "    'flatten_masks': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvf9rl_gWcM"
      },
      "source": [
        "Create an environment instance using this configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMllFumigWcM"
      },
      "outputs": [],
      "source": [
        "env = foundation.make_env_instance(**env_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNbvQ-5fgWcM"
      },
      "source": [
        "# 3. Interacting with the Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDeKzfZgWcM"
      },
      "source": [
        "### Agents\n",
        "\n",
        "The Agent class holds the state of agents in the simulation. Each Agent instance represents a _logical_ agent.\n",
        "\n",
        "_Note that this might be separate from a Policy model that lives outside the Scenario and controls the Agent's behavior._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKINEVrigWcM"
      },
      "outputs": [],
      "source": [
        "env.get_agent(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM41FWAGgWcM"
      },
      "source": [
        "### A random policy\n",
        "\n",
        "Now let's interact with the simulation.\n",
        "\n",
        "Each Agent needs to choose which actions to execute using a __policy__.\n",
        "\n",
        "Agents might not always be allowed to execute all actions. For instance, a mobile Agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\". This information is given by a mask, which is provided under ```obs[<agent_id_str>][\"action_mask\"]``` in the observation dictionary ```obs``` returned by the scenario.\n",
        "\n",
        "Let's use a random policy to step through the simulation. The methods below implement a random policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2yMxjkngWcM"
      },
      "outputs": [],
      "source": [
        "# Note: The code for sampling actions (this cell), and playing an episode (below) are general.\n",
        "# That is, it doesn't depend on the Scenario and Component classes used in the environment!\n",
        "\n",
        "def sample_random_action(agent, mask):\n",
        "    \"\"\"Sample random UNMASKED action(s) for agent.\"\"\"\n",
        "    # Return a list of actions: 1 for each action subspace\n",
        "    if agent.multi_action_mode:\n",
        "        split_masks = np.split(mask, agent.action_spaces.cumsum()[:-1])\n",
        "        return [np.random.choice(np.arange(len(m_)), p=m_/m_.sum()) for m_ in split_masks]\n",
        "\n",
        "    # Return a single action\n",
        "    else:\n",
        "        return np.random.choice(np.arange(agent.action_spaces), p=mask/mask.sum())\n",
        "\n",
        "def sample_random_actions(env, obs):\n",
        "    \"\"\"Samples random UNMASKED actions for each agent in obs.\"\"\"\n",
        "\n",
        "    actions = {\n",
        "        a_idx: sample_random_action(env.get_agent(a_idx), a_obs['action_mask'])\n",
        "        for a_idx, a_obs in obs.items()\n",
        "    }\n",
        "\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1GqtNt2gWcM"
      },
      "source": [
        "Now we're ready to interact with the simulation...\n",
        "\n",
        "First, environments can be put in an initial state by using __reset__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eHtzFUfgWcM"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHC6i5hTgWcM"
      },
      "source": [
        "Then, we call __step__ to advance the state and advance time by one tick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObP3xZzkgWcM"
      },
      "outputs": [],
      "source": [
        "actions = sample_random_actions(env, obs)\n",
        "obs, rew, done, info = env.step(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w96AabT3gWcN"
      },
      "source": [
        "Internally, the __step__ method composes several Components (which act almost like modular sub-Environments) that implement various agent affordances and environment dynamics. For more detailed information on Components and how to implement custom Component classes, see the advanced tutorial. For this tutorial, we will continue to inspect the information that __step__ returns and run a full episode in the simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8neDrea6gWcN"
      },
      "source": [
        "### Observation\n",
        "\n",
        "Each observation is a dictionary that contains information for the $N$ agents and (optionally) social planner (with id \"p\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF6_wmhtgWcN"
      },
      "outputs": [],
      "source": [
        "obs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxRw-S1tgWcN"
      },
      "source": [
        "For each agent, the agent-specific observation is a dictionary. Each Component can contribute information to the agent-specific observation. For instance, the Build Component contributes the\n",
        "\n",
        "- Build-build_payment (float)\n",
        "- Build-build_skill (int)\n",
        "\n",
        "fields, which are defined in the ```generate_observations``` method in [foundation/components/build.py](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/components/build.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xSdsU1KlgWcN"
      },
      "outputs": [],
      "source": [
        "for key, val in obs['0'].items():\n",
        "    print(\"{:50} {}\".format(key, type(val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxL_BerhgWcN"
      },
      "source": [
        "### Reward\n",
        "\n",
        "For each agent / planner, the reward dictionary contains a scalar reward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94hSMEjRgWcN"
      },
      "outputs": [],
      "source": [
        "for agent_idx, reward in rew.items():\n",
        "    print(\"{:2} {:.3f}\".format(agent_idx, reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL1jVinhgWcN"
      },
      "source": [
        "### Done\n",
        "\n",
        "The __done__ object is a dictionary that by default records whether all agents have seen the end of the episode. The default criterion for each agent is to 'stop' their episode once $H$ steps have been executed. Once an agent is 'done', they do not change their state anymore. So, while it's not currently implemented, this could be used to indicate that the episode has ended *for a specific Agent*.\n",
        "\n",
        "In general, this is useful for telling a Reinforcement Learning framework when to reset the environment and how to organize the trajectories of individual Agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqHE9MB9gWcN"
      },
      "outputs": [],
      "source": [
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgkG5EPwgWcN"
      },
      "source": [
        "### Info\n",
        "\n",
        "The __info__ object can record any auxiliary information from the simulator, which can be useful, e.g., for visualization. By default, this is empty. To modify this behavior, modify the step() method in [foundation/base/base_env.py](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/base/base_env.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-tUw1RrgWcN"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECkQty76gWcN"
      },
      "source": [
        "# 3. Sampling and Visualizing an Episode\n",
        "\n",
        "Let's step multiple times with this random policy and visualize the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAJi-uX3gWcN"
      },
      "outputs": [],
      "source": [
        "def do_plot(env, ax, fig):\n",
        "    \"\"\"Plots world state during episode sampling.\"\"\"\n",
        "    plotting.plot_env_state(env, ax)\n",
        "    ax.set_aspect('equal')\n",
        "    display.display(fig)\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "def play_random_episode(env, plot_every=100, do_dense_logging=False):\n",
        "    \"\"\"Plays an episode with randomly sampled actions.\n",
        "\n",
        "    Demonstrates gym-style API:\n",
        "        obs                  <-- env.reset(...)         # Reset\n",
        "        obs, rew, done, info <-- env.step(actions, ...) # Interaction loop\n",
        "\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    # Reset\n",
        "    obs = env.reset(force_dense_logging=do_dense_logging)\n",
        "\n",
        "    # Interaction loop (w/ plotting)\n",
        "    for t in range(env.episode_length):\n",
        "        actions = sample_random_actions(env, obs)\n",
        "        obs, rew, done, info = env.step(actions)\n",
        "\n",
        "        if ((t+1) % plot_every) == 0:\n",
        "            do_plot(env, ax, fig)\n",
        "\n",
        "    if ((t+1) % plot_every) != 0:\n",
        "        do_plot(env, ax, fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeB0YAEmgWcN"
      },
      "outputs": [],
      "source": [
        "play_random_episode(env, plot_every=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GNJvOnngWcN"
      },
      "source": [
        "We see four agents (indicated by a circled __\\*__) that move around in the 2-dimensional world. Light brown cells contain Stone, green cells contain Wood. Each agent can build Houses, indicated by corresponding colored cells. Water tiles (blue squares), which prevent movement, divide the map into four quadrants.\n",
        "\n",
        "Note: this is showing the same information as the image at the top of the tutorial -- it just uses a much more simplistic rendering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq4ps8LjgWcN"
      },
      "source": [
        "# Visualize using dense logging\n",
        "\n",
        "Environments built with Foundation provide a couple tools for logging. Perhaps the most useful are **dense logs**. When you reset the environment, you can tell it to create a dense log for the new episode. This will store Agent states at each point in time along with any Component-specific dense log information (say, about builds, trades, etc.) that the Components provide. In addition, it will periodically store a snapshot of the world state.\n",
        "\n",
        "We provide a few plotting tools that work well with the type of environment showcased here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipq1gKXYgWcR"
      },
      "outputs": [],
      "source": [
        "# Play another episode. This time, tell the environment to do dense logging\n",
        "play_random_episode(env, plot_every=100, do_dense_logging=True)\n",
        "\n",
        "# Grab the dense log from the env\n",
        "dense_log = env.previous_episode_dense_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P9HPKyzgWcR"
      },
      "outputs": [],
      "source": [
        "# Show the evolution of the world state from t=0 to t=200\n",
        "fig = plotting.vis_world_range(dense_log, t0=0, tN=200, N=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q478LDRgWcR"
      },
      "outputs": [],
      "source": [
        "# Show the evolution of the world state over the full episode\n",
        "fig = plotting.vis_world_range(dense_log, N=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2ONq5ccgWcR"
      },
      "outputs": [],
      "source": [
        "# Use the \"breakdown\" tool to visualize the world state, agent-wise quantities, movement, and trading events\n",
        "plotting.breakdown(dense_log);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_BtqXokgWcR"
      },
      "source": [
        "# Next\n",
        "\n",
        "Now that you've seen how to interact with the simulation and generate episodes, try the next tutorial ([economic_simulation_advanced.ipynb](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)) that explains how the simulation is composed of low-level Components and Entities. This structure enables flexible extensions of the economic simulation."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "948099c6ab02a15a055545cbca87e716aee9b3e6a51d0f68b03005577a92a5b6"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('ai-economist': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}